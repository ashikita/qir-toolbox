{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashikita/qir-toolbox/blob/main/make-wordcloud.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jXNn0e4o7CN",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# セル2: ワードクラウド生成（Colab最適化版）\n",
        "# ================================\n",
        "\n",
        "# ===== 設定ブロック（必要に応じて変更してください） =====\n",
        "INPUT_FILE = \"input.txt\"             # 1行に1タイトルが入ったUTF-8テキスト\n",
        "OUTPUT_PNG = \"wordcloud.png\"         # 出力画像（PNG）\n",
        "WC_WIDTH = 2000                      # 出力画像の幅（px）\n",
        "WC_HEIGHT = 1400                     # 出力画像の高さ（px）\n",
        "BACKGROUND_COLOR = \"white\"           # 背景色（white / black など）\n",
        "MAX_WORDS = 500                      # 出力する単語の最大数\n",
        "RANDOM_STATE = 42                    # レイアウトの乱数シード\n",
        "MIN_TOKEN_LEN = 2                    # 最小トークン長（英日とも）\n",
        "MIN_FREQUENCY = 2                    # 出現回数がこの値未満の語を除外\n",
        "NGRAM_N = 1                          # 1:ユニグラム, 2:バイグラム, 3:トライグラム\n",
        "EXTRA_STOPWORDS = {\n",
        "    # 追加で除外したい語をここに列挙（英語・日本語混在可）\n",
        "    # 例: \"study\", \"result\", \"結果\", \"研究\", \"日本\"\n",
        "}\n",
        "# \"auto\": 自動検出（推奨）。固定したい場合はフォントパスを文字列で指定。\n",
        "FONT_PATH = \"auto\"\n",
        "# ===============================================\n",
        "\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "import unicodedata\n",
        "from collections import Counter\n",
        "from itertools import tee\n",
        "\n",
        "# Colab でのファイルアップロード補助\n",
        "try:\n",
        "    from google.colab import files  # type: ignore\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    files = None\n",
        "    IN_COLAB = False\n",
        "\n",
        "# 依存ライブラリ\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "# Janome（推奨：日本語の高精度トークナイズ）\n",
        "try:\n",
        "    from janome.tokenizer import Tokenizer as JanomeTokenizer\n",
        "    _janome = JanomeTokenizer(wakati=False)\n",
        "except Exception:\n",
        "    _janome = None\n",
        "\n",
        "# TinySegmenter（フォールバック用）\n",
        "try:\n",
        "    from tinysegmenter import TinySegmenter\n",
        "    _tinyseg = TinySegmenter()\n",
        "except Exception:\n",
        "    _tinyseg = None\n",
        "\n",
        "# 画像表示用（任意）\n",
        "from IPython.display import display\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "# ========== ユーティリティ ==========\n",
        "_CJK_RE = re.compile(r\"[\\u3040-\\u30FF\\u3400-\\u4DBF\\u4E00-\\u9FFF]\")\n",
        "_NUM_RE = re.compile(r\"^[0-9]+$\")\n",
        "_SYM_RE = re.compile(r\"^[_\\W]+$\", flags=re.UNICODE)\n",
        "_EN_TOKEN_RE = re.compile(r\"[A-Za-z][A-Za-z']+\")\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    \"\"\"Unicode正規化（NFKC）と制御文字の簡易除去。\"\"\"\n",
        "    t = unicodedata.normalize(\"NFKC\", text)\n",
        "    t = re.sub(r\"[\\u0000-\\u001F\\u007F\\u200B-\\u200F\\u202A-\\u202E]\", \" \", t)\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "    return t\n",
        "\n",
        "def contains_cjk(text: str) -> bool:\n",
        "    return bool(_CJK_RE.search(text))\n",
        "\n",
        "def english_tokens(text: str):\n",
        "    t = text.lower()\n",
        "    return _EN_TOKEN_RE.findall(t)\n",
        "\n",
        "def japanese_tokens(text: str):\n",
        "    # Janome がある場合：名詞/動詞/形容詞の基本形を収集（語彙の正規化）\n",
        "    if _janome is not None:\n",
        "        toks = []\n",
        "        for token in _janome.tokenize(text):\n",
        "            pos = token.part_of_speech.split(\",\")[0]  # 先頭品詞\n",
        "            base = token.base_form if token.base_form != \"*\" else token.surface\n",
        "            if pos in (\"名詞\", \"動詞\", \"形容詞\"):\n",
        "                toks.append(base)\n",
        "        return toks\n",
        "\n",
        "    # TinySegmenter がある場合：単純分割\n",
        "    if _tinyseg is not None:\n",
        "        return _tinyseg.tokenize(text)\n",
        "\n",
        "    # さらにフォールバック：漢字/かな/カナの連続\n",
        "    return re.findall(r\"[一-龥々〆ヵヶぁ-んァ-ヴー]+\", text)\n",
        "\n",
        "def make_ngrams(tokens, n=2):\n",
        "    \"\"\"n-gram のタプルを 'token1_token2' の文字列へ。\"\"\"\n",
        "    if n <= 1:\n",
        "        return tokens\n",
        "    iters = tee(tokens, n)\n",
        "    for i, it in enumerate(iters):\n",
        "        for _ in range(i):\n",
        "            next(it, None)\n",
        "    return [\"_\".join(t) for t in zip(*iters)]\n",
        "\n",
        "def load_lines(path: str):\n",
        "    with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                yield line\n",
        "\n",
        "\n",
        "# ========== ストップワード（英・日） ==========\n",
        "EN_STOP = {\n",
        "    \"a\",\"an\",\"the\",\"and\",\"or\",\"but\",\"if\",\"then\",\"else\",\"when\",\"while\",\"for\",\"of\",\"in\",\"on\",\"at\",\n",
        "    \"to\",\"from\",\"by\",\"with\",\"without\",\"about\",\"across\",\"after\",\"before\",\"during\",\"within\",\"between\",\n",
        "    \"is\",\"am\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\"do\",\"does\",\"did\",\"doing\",\"have\",\"has\",\"had\",\n",
        "    \"having\",\"can\",\"could\",\"may\",\"might\",\"must\",\"should\",\"would\",\"will\",\"shall\",\n",
        "    \"this\",\"that\",\"these\",\"those\",\"it\",\"its\",\"as\",\"not\",\"no\",\"yes\",\"we\",\"you\",\"he\",\"she\",\"they\",\"them\",\n",
        "    \"their\",\"there\",\"here\",\"such\",\"than\",\"via\",\"per\",\"etc\",\"i\",\"our\",\"us\",\"your\",\"yours\",\"his\",\"her\",\n",
        "    \"into\",\"out\",\"over\",\"under\",\"again\",\"new\",\"based\",\"using\",\"use\",\"used\",\"study\",\"analysis\",\"results\",\n",
        "    \"result\",\"method\",\"methods\",\"approach\",\"approaches\",\"paper\",\"article\",\"report\",\"case\",\"cases\",\n",
        "    \"effect\",\"effects\",\"evidence\",\"evaluation\",\"overview\",\"review\",\"reviews\",\"systematic\",\"meta\",\n",
        "    \"toward\",\"towards\",\"impact\",\"impacts\",\"improving\",\"improvement\",\"improve\",\n",
        "    \"model\",\"models\",\"modelling\",\"modeling\",\"data\",\"dataset\",\"datasets\",\"large\",\"small\",\"novel\",\n",
        "    \"among\",\"across\",\"more\",\"most\",\"less\",\"least\"\n",
        "}\n",
        "\n",
        "JA_STOP = {\n",
        "    \"こと\",\"もの\",\"ところ\",\"ため\",\"よう\",\"られ\",\"れる\",\"ない\",\"ある\",\"いる\",\"おり\",\"なる\",\"する\",\"できる\",\n",
        "    \"的\",\"的な\",\"的に\",\"における\",\"において\",\"及び\",\"および\",\"ならびに\",\n",
        "    \"これ\",\"それ\",\"あれ\",\"どれ\",\"ここ\",\"そこ\",\"あそこ\",\"どこ\",\"この\",\"その\",\"あの\",\"どの\",\n",
        "    \"そして\",\"また\",\"さらに\",\"しかし\",\"一方\",\"など\",\"等\",\"等の\",\"等を\",\"等に\",\"等で\",\"等と\",\n",
        "    \"例\",\"場合\",\"結果\",\"研究\",\"検討\",\"報告\",\"考察\",\"課題\",\"手法\",\"方法\",\"比較\",\n",
        "    \"影響\",\"実験\",\"解析\",\"分析\",\"評価\",\"事例\",\"概要\",\"総説\",\"序論\",\"序説\",\"序\",\"序文\",\"序章\",\n",
        "    \"新た\",\"新しい\",\"新規\",\"一\",\"二\",\"三\",\"四\",\"五\",\"六\",\"七\",\"八\",\"九\",\"十\",\"第\",\"年\",\"月\",\"日\",\n",
        "}\n",
        "\n",
        "STOPWORDS = set(EN_STOP) | set(JA_STOP) | set(EXTRA_STOPWORDS)\n",
        "\n",
        "\n",
        "# ========== フォント自動検出（Colab向け） ==========\n",
        "def find_noto_cjk_font() -> str | None:\n",
        "    \"\"\"Colab にインストールされた NotoSansCJK の TTC/OTF を探す。\"\"\"\n",
        "    if isinstance(FONT_PATH, str) and FONT_PATH not in (None, \"\", \"auto\"):\n",
        "        return FONT_PATH if os.path.exists(FONT_PATH) else None\n",
        "\n",
        "    # よくあるパス（fonts-noto-cjk パッケージ）\n",
        "    common_paths = [\n",
        "        \"/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc\",\n",
        "        \"/usr/share/fonts/opentype/noto/NotoSansCJKjp-Regular.otf\",\n",
        "        \"/usr/share/fonts/opentype/noto/NotoSansCJKjp-Regular.ttc\",\n",
        "        \"/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc\",\n",
        "    ]\n",
        "    for p in common_paths:\n",
        "        if os.path.exists(p):\n",
        "            return p\n",
        "\n",
        "    # font_manager から検索\n",
        "    names = (\"NotoSansCJK\", \"Noto Sans CJK\", \"Noto Sans CJK JP\", \"NotoSansCJKjp\")\n",
        "    for f in fm.fontManager.ttflist:\n",
        "        fname = getattr(f, \"fname\", \"\")\n",
        "        if not fname:\n",
        "            continue\n",
        "        base = os.path.basename(fname)\n",
        "        if any(n.replace(\" \", \"\") in base.replace(\" \", \"\") for n in names):\n",
        "            if os.path.exists(fname):\n",
        "                return fname\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "# ========== 前処理 & トークン化 ==========\n",
        "def preprocess_and_tokenize(line: str):\n",
        "    \"\"\"1行（1タイトル）から英語・日本語トークンを抽出し結合。\"\"\"\n",
        "    text = normalize_text(line)\n",
        "\n",
        "    # 英語トークン\n",
        "    en = english_tokens(text)\n",
        "\n",
        "    # 日本語トークン\n",
        "    ja = japanese_tokens(text) if contains_cjk(text) else []\n",
        "\n",
        "    tokens = en + ja\n",
        "\n",
        "    # 記号・数字のみや短すぎる語を除外\n",
        "    clean = []\n",
        "    for tok in tokens:\n",
        "        tok = tok.strip()\n",
        "        if not tok:\n",
        "            continue\n",
        "        if _NUM_RE.fullmatch(tok):\n",
        "            continue\n",
        "        if _SYM_RE.fullmatch(tok):\n",
        "            continue\n",
        "        if len(tok) < MIN_TOKEN_LEN:\n",
        "            continue\n",
        "        clean.append(tok)\n",
        "\n",
        "    return clean\n",
        "\n",
        "\n",
        "# ========== メイン処理 ==========\n",
        "def main():\n",
        "    # 入力が無ければ Colab のアップロードダイアログを開く\n",
        "    if not os.path.exists(INPUT_FILE) and IN_COLAB and files is not None:\n",
        "        print(f\"'{INPUT_FILE}' が見つかりません。ダイアログからアップロードしてください。\")\n",
        "        uploaded = files.upload()\n",
        "        # アップロード名が異なる場合は最初のファイルを INPUT_FILE として扱う\n",
        "        if INPUT_FILE not in uploaded:\n",
        "            # 最初のキーを採用\n",
        "            alt = next(iter(uploaded.keys()))\n",
        "            os.rename(alt, INPUT_FILE)\n",
        "            print(f\"アップロードされた '{alt}' を '{INPUT_FILE}' として使用します。\")\n",
        "\n",
        "    if not os.path.exists(INPUT_FILE):\n",
        "        raise FileNotFoundError(f\"{INPUT_FILE} が見つかりません。パスを確認してください。\")\n",
        "\n",
        "    freq = Counter()\n",
        "\n",
        "    # 読み込み & トークン化\n",
        "    for line in load_lines(INPUT_FILE):\n",
        "        tokens = preprocess_and_tokenize(line)\n",
        "        if NGRAM_N and NGRAM_N > 1:\n",
        "            tokens = make_ngrams(tokens, n=NGRAM_N)\n",
        "        tokens = [t for t in tokens if t not in STOPWORDS]\n",
        "        freq.update(tokens)\n",
        "\n",
        "    # 最低頻度フィルタ\n",
        "    if MIN_FREQUENCY > 1:\n",
        "        freq = Counter({k: v for k, v in freq.items() if v >= MIN_FREQUENCY})\n",
        "\n",
        "    if not freq:\n",
        "        raise ValueError(\"有効なトークンが得られませんでした。STOPWORDS, MIN_TOKEN_LEN, MIN_FREQUENCY を見直してください。\")\n",
        "\n",
        "    # フォント決定\n",
        "    font_path = find_noto_cjk_font()\n",
        "    if font_path is None:\n",
        "        print(\n",
        "            \"警告: 日本語フォントを自動検出できませんでした。\"\n",
        "            \" 'FONT_PATH' に日本語フォント(TTC/TTF/OTF)のパスを指定してください。\"\n",
        "        )\n",
        "\n",
        "    # WordCloud 生成\n",
        "    wc = WordCloud(\n",
        "        width=WC_WIDTH,\n",
        "        height=WC_HEIGHT,\n",
        "        background_color=BACKGROUND_COLOR,\n",
        "        max_words=MAX_WORDS,\n",
        "        font_path=font_path,        # 日本語表示に重要\n",
        "        prefer_horizontal=0.9,\n",
        "        random_state=RANDOM_STATE,\n",
        "        collocations=False,         # n-gramは自前で生成する想定\n",
        "        regexp=None\n",
        "    ).generate_from_frequencies(freq)\n",
        "\n",
        "    # 画像保存（PNG）\n",
        "    wc.to_file(OUTPUT_PNG)\n",
        "\n",
        "    # 概要表示\n",
        "    top_items = freq.most_common(20)\n",
        "    print(f\"出力: {OUTPUT_PNG}\")\n",
        "    print(f\"語彙数: {len(freq)} / 最大語数: {MAX_WORDS}\")\n",
        "    print(\"上位20語:\")\n",
        "    for w, c in top_items:\n",
        "        print(f\"{w}\\t{c}\")\n",
        "\n",
        "    # Colab 上でプレビュー\n",
        "    try:\n",
        "        display(Image.open(OUTPUT_PNG))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "id": "4jXNn0e4o7CN"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}