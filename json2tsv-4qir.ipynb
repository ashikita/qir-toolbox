{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashikita/qir-toolbox/blob/main/json2tsv-4qir.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jXNn0e4o7CN",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "機能:\n",
        "- 作業ディレクトリ内の *.json をすべて読み込み、論文メタデータをTAB区切りの一括登録用フォーマットに整形して output.txt を生成\n",
        "- 1論文1行、先頭行に可変列を含むヘッダを出力\n",
        "- かな漢字/アルファベットに基づき xml:lang を簡易判定\n",
        "- 共著者と所属の可変数に対応し、列ずれが起こらないように最大数に合わせてTABを補完\n",
        "- 指示に沿った初期値は設定ブロックで変更可能\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import glob\n",
        "from typing import List, Dict, Any, Tuple\n",
        "\n",
        "# =========================\n",
        "# 設定ブロック（必要に応じて編集）\n",
        "# =========================\n",
        "DEFAULTS = {\n",
        "    \"access_rights\": \"110\",  # /dcterms:accessRights#1\n",
        "    \"rights_label\": \"Creative Commons Attribution 4.0 International\",  # /dc:rights#1\n",
        "    \"rights_lang\": \"en\",  # /dc:rights#1@xml:lang\n",
        "    \"rights_uri\": \"https://creativecommons.org/licenses/by/4.0/\",  # /dc:rights#1@rdf:resource\n",
        "    \"publisher_en\": \"Transdisciplinary Research and Education Center for Green Technologies, Kyushu University\",  # /dc:publisher#1\n",
        "    \"publisher_ja\": \"九州大学グリーンテクノロジー研究教育センター\",  # /dc:publisher#2\n",
        "    \"issued_date\": \"2026-01\",  # /datacite:date#1\n",
        "    \"issued_date_type\": \"Issued\",  # /datacite:date#1@dateType\n",
        "    \"language_three_letter\": \"eng\",  # /dc:language#1\n",
        "    \"contents_type\": \"1207000000\",  # /local:contentsType#1 (journal article)\n",
        "    \"version\": \"VoR\",  # /oaire:version#1\n",
        "    \"peer_reviewed\": \"refereed\",  # /local:peerReviewed#1\n",
        "    \"pissn\": \"2189-0420\",  # /jpcoar:sourceIdentifier#1\n",
        "    \"pissn_type\": \"PISSN\",\n",
        "    \"eissn\": \"2432-5953\",  # /jpcoar:sourceIdentifier#2\n",
        "    \"eissn_type\": \"eISSN\",\n",
        "    \"source_title\": \"evergreen\",  # /local:sourceTitle#1\n",
        "    \"volume\": \"12\",  # /jpcoar:volume#1\n",
        "    \"issue\": \"4\",  # /jpcoar:issue#1\n",
        "    \"subject_scheme\": \"Other\",  # /jpcoar:subject#n@subjectScheme\n",
        "    \"description_type\": \"Abstract\",  # /datacite:description#1@descriptionType\n",
        "}\n",
        "\n",
        "INPUT_GLOB_PATTERN = \"*.json\"\n",
        "OUTPUT_FILE = \"output.txt\"\n",
        "ENCODING = \"utf-8\"\n",
        "\n",
        "# =========================\n",
        "# ユーティリティ\n",
        "# =========================\n",
        "\n",
        "JP_REGEX = re.compile(r\"[\\u3040-\\u30ff\\u3400-\\u4dbf\\u4e00-\\u9fff\\uff66-\\uff9f]\")\n",
        "EN_REGEX = re.compile(r\"[A-Za-z]\")\n",
        "ORCID_REGEX = re.compile(r\"^\\d{4}-\\d{4}-\\d{4}-\\d{3}[\\dX]$\")\n",
        "\n",
        "def detect_lang(text: str) -> str:\n",
        "    \"\"\"かな・漢字（含むカタカナ）→ 'ja'、アルファベット → 'en'、それ以外は空値\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    if JP_REGEX.search(text):\n",
        "        return \"ja\"\n",
        "    if EN_REGEX.search(text):\n",
        "        return \"en\"\n",
        "    return \"\"\n",
        "\n",
        "def sanitize_value(v: Any) -> str:\n",
        "    \"\"\"タブ・改行を空白化し、前後空白をトリム\"\"\"\n",
        "    if v is None:\n",
        "        return \"\"\n",
        "    s = str(v)\n",
        "    s = s.replace(\"\\t\", \" \").replace(\"\\r\\n\", \" \").replace(\"\\n\", \" \")\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    return s.strip()\n",
        "\n",
        "def valid_orcid(oid: str) -> bool:\n",
        "    return bool(ORCID_REGEX.match(oid.strip())) if oid else False\n",
        "\n",
        "def split_affiliation_indices(raw_idx) -> List[str]:\n",
        "    \"\"\"所属インデックスが '1' / '1,2' / '1; 2' / [1,2] など様々な形式に対応\"\"\"\n",
        "    if raw_idx is None:\n",
        "        return []\n",
        "    if isinstance(raw_idx, list):\n",
        "        return [str(x).strip() for x in raw_idx if str(x).strip()]\n",
        "    s = str(raw_idx)\n",
        "    parts = re.split(r\"[,\\s;]+\", s)\n",
        "    return [p for p in (x.strip() for x in parts) if p]\n",
        "\n",
        "def build_orcid_map(orcid_list: Any) -> Dict[str, str]:\n",
        "    \"\"\"orcid項目（配列）から {author_name: orcid_id} の辞書を構築\"\"\"\n",
        "    m: Dict[str, str] = {}\n",
        "    if isinstance(orcid_list, list):\n",
        "        for item in orcid_list:\n",
        "            try:\n",
        "                name = sanitize_value(item.get(\"author_name\"))\n",
        "                oid = sanitize_value(item.get(\"orcid_id\"))\n",
        "                if name and valid_orcid(oid):\n",
        "                    m[name] = oid\n",
        "            except Exception:\n",
        "                continue\n",
        "    return m\n",
        "\n",
        "def build_affiliation_map(aff_list: Any) -> Dict[str, str]:\n",
        "    \"\"\"affiliation配列から {index: name} 辞書を構築\"\"\"\n",
        "    m: Dict[str, str] = {}\n",
        "    if isinstance(aff_list, list):\n",
        "        for item in aff_list:\n",
        "            try:\n",
        "                idx = sanitize_value(item.get(\"affiliation_index\"))\n",
        "                name = sanitize_value(item.get(\"affiliation_name\"))\n",
        "                if idx and name:\n",
        "                    m[idx] = name\n",
        "            except Exception:\n",
        "                continue\n",
        "    return m\n",
        "\n",
        "def parse_json_file(path: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    JSONファイルを柔軟に解釈:\n",
        "    - 単一オブジェクト\n",
        "    - 配列\n",
        "    - 改行区切りの複数JSONオブジェクト\n",
        "    - 連結JSON（{}{}{}...）を波括弧バランスで分割\n",
        "    \"\"\"\n",
        "    with open(path, \"r\", encoding=ENCODING) as f:\n",
        "        txt = f.read().strip()\n",
        "    if not txt:\n",
        "        return []\n",
        "\n",
        "    # まずは通常のJSONとして試行\n",
        "    try:\n",
        "        obj = json.loads(txt)\n",
        "        if isinstance(obj, list):\n",
        "            return [x for x in obj if isinstance(x, dict)]\n",
        "        if isinstance(obj, dict):\n",
        "            return [obj]\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # 行ごと（JSON Lines）を試行\n",
        "    records = []\n",
        "    for line in txt.splitlines():\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        try:\n",
        "            obj = json.loads(line)\n",
        "            if isinstance(obj, dict):\n",
        "                records.append(obj)\n",
        "        except Exception:\n",
        "            records = []\n",
        "            break\n",
        "    if records:\n",
        "        return records\n",
        "\n",
        "    # 波括弧バランスで分割\n",
        "    parts = []\n",
        "    depth = 0\n",
        "    start = None\n",
        "    for i, ch in enumerate(txt):\n",
        "        if ch == \"{\":\n",
        "            if depth == 0:\n",
        "                start = i\n",
        "            depth += 1\n",
        "        elif ch == \"}\":\n",
        "            depth -= 1\n",
        "            if depth == 0 and start is not None:\n",
        "                parts.append(txt[start:i+1])\n",
        "                start = None\n",
        "    out = []\n",
        "    for p in parts:\n",
        "        try:\n",
        "            obj = json.loads(p)\n",
        "            if isinstance(obj, dict):\n",
        "                out.append(obj)\n",
        "        except Exception:\n",
        "            # 最後の手段: 末尾のカンマ欠落など軽微な不備は諦めてスキップ\n",
        "            continue\n",
        "    return out\n",
        "\n",
        "# =========================\n",
        "# データ一次走査: 最大列数の算出\n",
        "# =========================\n",
        "\n",
        "json_files = sorted(glob.glob(INPUT_GLOB_PATTERN))\n",
        "all_records: List[Dict[str, Any]] = []\n",
        "if not json_files:\n",
        "    # 出力だけ作成（空ヘッダは無意味なので、最低限の固定列のみ）\n",
        "    json_files = []\n",
        "\n",
        "for fp in json_files:\n",
        "    try:\n",
        "        recs = parse_json_file(fp)\n",
        "        all_records.extend(recs)\n",
        "    except Exception:\n",
        "        # 読み取り不能ファイルはスキップ\n",
        "        continue\n",
        "\n",
        "# レコードを内部表現に正規化しつつ、列の最大数を計測\n",
        "normalized: List[Dict[str, Any]] = []\n",
        "max_creators = 0\n",
        "# 各著者位置(i)ごとの最大 affiliation 数（1-basedではなく0-based保持）\n",
        "max_affils_per_pos: List[int] = []\n",
        "max_subjects = 0\n",
        "\n",
        "for rec in all_records:\n",
        "    title = sanitize_value(rec.get(\"title\", \"\"))\n",
        "\n",
        "    # orcid と affiliation の辞書化\n",
        "    orcid_map = build_orcid_map(rec.get(\"orcid\"))\n",
        "    aff_map = build_affiliation_map(rec.get(\"affiliation\"))\n",
        "\n",
        "    # 著者\n",
        "    authors_raw = rec.get(\"author\", []) or rec.get(\"authors\", [])\n",
        "    authors_norm = []\n",
        "    if isinstance(authors_raw, list):\n",
        "        for a in authors_raw:\n",
        "            if not isinstance(a, dict):\n",
        "                continue\n",
        "            name = sanitize_value(a.get(\"author_name\") or a.get(\"name\"))\n",
        "            if not name:\n",
        "                continue\n",
        "            aff_idx_list = split_affiliation_indices(a.get(\"affiliation_index\"))\n",
        "            aff_names = [sanitize_value(aff_map.get(idx, \"\")) for idx in aff_idx_list]\n",
        "            aff_names = [x for x in aff_names if x]  # 空は除外\n",
        "            authors_norm.append({\n",
        "                \"name\": name,\n",
        "                \"name_lang\": detect_lang(name),\n",
        "                \"orcid\": sanitize_value(orcid_map.get(name, \"\")),\n",
        "                \"affiliations\": [{\"name\": an, \"lang\": detect_lang(an)} for an in aff_names]\n",
        "            })\n",
        "\n",
        "    # subjects（keyword）\n",
        "    kw = rec.get(\"keyword\") or rec.get(\"keywords\")\n",
        "    subjects = []\n",
        "    if isinstance(kw, list):\n",
        "        for s in kw:\n",
        "            val = sanitize_value(s)\n",
        "            if val:\n",
        "                subjects.append({\"value\": val, \"lang\": detect_lang(val)})\n",
        "\n",
        "    # 概要\n",
        "    desc = sanitize_value(rec.get(\"Abstract\") or rec.get(\"abstract\") or \"\")\n",
        "\n",
        "    item = {\n",
        "        \"title\": title,\n",
        "        \"title_lang\": detect_lang(title),\n",
        "        \"authors\": authors_norm,\n",
        "        \"subjects\": subjects,\n",
        "        \"description\": desc,\n",
        "        \"description_lang\": detect_lang(desc),\n",
        "    }\n",
        "    normalized.append(item)\n",
        "\n",
        "    # 最大数更新\n",
        "    n_creators = len(authors_norm)\n",
        "    if n_creators > max_creators:\n",
        "        max_creators = n_creators\n",
        "    # 著者位置ごとの所属最大数\n",
        "    for i, au in enumerate(authors_norm):\n",
        "        aff_count = len(au.get(\"affiliations\", []))\n",
        "        if i >= len(max_affils_per_pos):\n",
        "            max_affils_per_pos.extend([0] * (i + 1 - len(max_affils_per_pos)))\n",
        "        if aff_count > max_affils_per_pos[i]:\n",
        "            max_affils_per_pos[i] = aff_count\n",
        "    # subject最大数\n",
        "    if len(subjects) > max_subjects:\n",
        "        max_subjects = len(subjects)\n",
        "\n",
        "# =========================\n",
        "# ヘッダ構築\n",
        "# =========================\n",
        "\n",
        "columns: List[str] = []\n",
        "# 1) タイトル\n",
        "columns.append(\"/dc:title#1\")\n",
        "columns.append(\"/dc:title#1@xml:lang\")\n",
        "\n",
        "# 2) クリエータ（可変）\n",
        "for i in range(1, max_creators + 1):\n",
        "    columns.append(f\"/jpcoar:creator#{i}\")  # 値は空（ヘッダ的）\n",
        "    columns.append(f\"/jpcoar:creator#{i}/jpcoar:nameIdentifier#1\")\n",
        "    columns.append(f\"/jpcoar:creator#{i}/jpcoar:nameIdentifier#1@nameIdentifierScheme\")\n",
        "    columns.append(f\"/jpcoar:creator#{i}/jpcoar:nameIdentifier#1@nameIdentifierURI\")\n",
        "    columns.append(f\"/jpcoar:creator#{i}/jpcoar:creatorName#1\")\n",
        "    columns.append(f\"/jpcoar:creator#{i}/jpcoar:creatorName#1@xml:lang\")\n",
        "    # 所属（著者位置ごとの最大数）\n",
        "    max_aff = max_affils_per_pos[i-1] if i-1 < len(max_affils_per_pos) else 0\n",
        "    for j in range(1, max_aff + 1):\n",
        "        columns.append(f\"/jpcoar:creator#{i}/jpcoar:affiliation#{j}\")  # 値は空（ヘッダ的）\n",
        "        columns.append(f\"/jpcoar:creator#{i}/jpcoar:affiliation#{j}/jpcoar:affiliationName#1\")\n",
        "        columns.append(f\"/jpcoar:creator#{i}/jpcoar:affiliation#{j}/jpcoar:affiliationName#1@xml:lang\")\n",
        "\n",
        "# 3) アクセス権\n",
        "columns.append(\"/dcterms:accessRights#1\")\n",
        "columns.append(\"/dcterms:accessRights#1@rdf:resource\")\n",
        "\n",
        "# 4) 権利情報\n",
        "columns.append(\"/dc:rights#1\")\n",
        "columns.append(\"/dc:rights#1@xml:lang\")\n",
        "columns.append(\"/dc:rights#1@rdf:resource\")\n",
        "\n",
        "# 5) 主題（可変）\n",
        "for k in range(1, max_subjects + 1):\n",
        "    columns.append(f\"/jpcoar:subject#{k}\")\n",
        "    columns.append(f\"/jpcoar:subject#{k}@xml:lang\")\n",
        "    columns.append(f\"/jpcoar:subject#{k}@subjectScheme\")\n",
        "\n",
        "# 6) 要約\n",
        "columns.append(\"/datacite:description#1\")\n",
        "columns.append(\"/datacite:description#1@xml:lang\")\n",
        "columns.append(\"/datacite:description#1@descriptionType\")\n",
        "\n",
        "# 7) 出版者(英/日)\n",
        "columns.append(\"/dc:publisher#1\")\n",
        "columns.append(\"/dc:publisher#1@xml:lang\")\n",
        "columns.append(\"/dc:publisher#2\")\n",
        "columns.append(\"/dc:publisher#2@xml:lang\")\n",
        "\n",
        "# 8) 日付（Issued）\n",
        "columns.append(\"/datacite:date#1\")\n",
        "columns.append(\"/datacite:date#1@dateType\")\n",
        "\n",
        "# 9) 言語\n",
        "columns.append(\"/dc:language#1\")\n",
        "\n",
        "# 10) コンテンツタイプ\n",
        "columns.append(\"/local:contentsType#1\")\n",
        "\n",
        "# 11) バージョン/査読\n",
        "columns.append(\"/oaire:version#1\")\n",
        "columns.append(\"/local:peerReviewed#1\")\n",
        "\n",
        "# 12) ソース識別子等\n",
        "columns.append(\"/jpcoar:sourceIdentifier#1\")\n",
        "columns.append(\"/jpcoar:sourceIdentifier#1@identifierType\")\n",
        "columns.append(\"/jpcoar:sourceIdentifier#2\")\n",
        "columns.append(\"/jpcoar:sourceIdentifier#2@identifierType\")\n",
        "columns.append(\"/local:sourceTitle#1\")\n",
        "columns.append(\"/jpcoar:volume#1\")\n",
        "columns.append(\"/jpcoar:issue#1\")\n",
        "\n",
        "# =========================\n",
        "# 行データ生成\n",
        "# =========================\n",
        "\n",
        "def row_for_item(item: Dict[str, Any]) -> List[str]:\n",
        "    row: List[str] = []\n",
        "    # タイトル\n",
        "    row.append(item.get(\"title\", \"\"))\n",
        "    row.append(item.get(\"title_lang\", \"\"))\n",
        "\n",
        "    # クリエータ\n",
        "    authors = item.get(\"authors\", [])\n",
        "    for i in range(max_creators):\n",
        "        if i < len(authors):\n",
        "            au = authors[i]\n",
        "            # ヘッダ的列（空）\n",
        "            row.append(\"\")\n",
        "            oid = au.get(\"orcid\") or \"\"\n",
        "            row.append(oid if valid_orcid(oid) else \"\")\n",
        "            row.append(\"ORCID\" if valid_orcid(oid) else \"\")\n",
        "            row.append(f\"https://orcid.org/{oid}\" if valid_orcid(oid) else \"\")\n",
        "            row.append(au.get(\"name\", \"\"))\n",
        "            row.append(au.get(\"name_lang\", \"\"))\n",
        "\n",
        "            # 所属（最大数に合わせて埋める）\n",
        "            affs = au.get(\"affiliations\", [])\n",
        "            max_aff = max_affils_per_pos[i] if i < len(max_affils_per_pos) else 0\n",
        "            for j in range(max_aff):\n",
        "                if j < len(affs):\n",
        "                    af = affs[j]\n",
        "                    row.append(\"\")  # affiliation ヘッダ列（空）\n",
        "                    row.append(af.get(\"name\", \"\"))\n",
        "                    row.append(af.get(\"lang\", \"\"))\n",
        "                else:\n",
        "                    row.extend([\"\", \"\", \"\"])\n",
        "        else:\n",
        "            # 著者が存在しない位置 → その位置の全列を空で補完\n",
        "            row.append(\"\")  # creator#i ヘッダ\n",
        "            row.append(\"\")  # nameIdentifier\n",
        "            row.append(\"\")  # nameIdentifierScheme\n",
        "            row.append(\"\")  # nameIdentifierURI\n",
        "            row.append(\"\")  # creatorName\n",
        "            row.append(\"\")  # creatorName@xml:lang\n",
        "            max_aff = max_affils_per_pos[i] if i < len(max_affils_per_pos) else 0\n",
        "            for _ in range(max_aff):\n",
        "                row.extend([\"\", \"\", \"\"])  # affiliation ヘッダ/名称/lang\n",
        "\n",
        "    # アクセス権\n",
        "    row.append(DEFAULTS[\"access_rights\"])\n",
        "    row.append(\"\")  # @rdf:resource 空\n",
        "\n",
        "    # 権利情報\n",
        "    row.append(DEFAULTS[\"rights_label\"])\n",
        "    row.append(DEFAULTS[\"rights_lang\"])\n",
        "    row.append(DEFAULTS[\"rights_uri\"])\n",
        "\n",
        "    # 主題\n",
        "    subs = item.get(\"subjects\", [])\n",
        "    for k in range(max_subjects):\n",
        "        if k < len(subs):\n",
        "            s = subs[k]\n",
        "            row.append(s.get(\"value\", \"\"))\n",
        "            row.append(s.get(\"lang\", \"\"))\n",
        "            row.append(DEFAULTS[\"subject_scheme\"])\n",
        "        else:\n",
        "            row.extend([\"\", \"\", \"\"])\n",
        "\n",
        "    # 要約\n",
        "    row.append(item.get(\"description\", \"\"))\n",
        "    row.append(item.get(\"description_lang\", \"\"))\n",
        "    row.append(DEFAULTS[\"description_type\"])\n",
        "\n",
        "    # 出版者（英/日）\n",
        "    pub1 = DEFAULTS[\"publisher_en\"]\n",
        "    pub2 = DEFAULTS[\"publisher_ja\"]\n",
        "    row.append(pub1)\n",
        "    row.append(detect_lang(pub1))\n",
        "    row.append(pub2)\n",
        "    row.append(detect_lang(pub2))\n",
        "\n",
        "    # 日付\n",
        "    row.append(DEFAULTS[\"issued_date\"])\n",
        "    row.append(DEFAULTS[\"issued_date_type\"])\n",
        "\n",
        "    # 言語\n",
        "    row.append(DEFAULTS[\"language_three_letter\"])\n",
        "\n",
        "    # コンテンツタイプ\n",
        "    row.append(DEFAULTS[\"contents_type\"])\n",
        "\n",
        "    # バージョン/査読\n",
        "    row.append(DEFAULTS[\"version\"])\n",
        "    row.append(DEFAULTS[\"peer_reviewed\"])\n",
        "\n",
        "    # 出典関連\n",
        "    row.append(DEFAULTS[\"pissn\"])\n",
        "    row.append(DEFAULTS[\"pissn_type\"])\n",
        "    row.append(DEFAULTS[\"eissn\"])\n",
        "    row.append(DEFAULTS[\"eissn_type\"])\n",
        "    row.append(DEFAULTS[\"source_title\"])\n",
        "    row.append(DEFAULTS[\"volume\"])\n",
        "    row.append(DEFAULTS[\"issue\"])\n",
        "\n",
        "    return [sanitize_value(x) for x in row]\n",
        "\n",
        "# =========================\n",
        "# 書き出し\n",
        "# =========================\n",
        "\n",
        "with open(OUTPUT_FILE, \"w\", encoding=ENCODING, newline=\"\") as fw:\n",
        "    # ヘッダ\n",
        "    fw.write(\"\\t\".join(columns) + \"\\n\")\n",
        "    # レコード\n",
        "    for it in normalized:\n",
        "        fw.write(\"\\t\".join(row_for_item(it)) + \"\\n\")\n",
        "\n",
        "print(f\"処理が完了しました。{len(normalized)}件のレコードを'{OUTPUT_FILE}'に出力しました。\")"
      ],
      "id": "4jXNn0e4o7CN"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}